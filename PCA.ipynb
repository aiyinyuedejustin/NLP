{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQ2bfvFLvMCc"
      },
      "source": [
        "# table of contents\n",
        "# 1. PCA on digits for visualization \n",
        "# 2. PCA on digits for compression\n",
        "# 3. PCA on digits improve classification # 4. K-means clustering on digits\n",
        "\n",
        "import numpy as np\n",
        "import pylab as py\n",
        "\n",
        "%matplotlib inline\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFVhE4Cav0ZO"
      },
      "source": [
        "# digit recognition setup...\n",
        "\n",
        "from sklearn.datasets import load_digits \n",
        "digits = load_digits()\n",
        "\n",
        "X, y = digits.data, digits.target\n",
        "print(\"data shape: %r, target shape: %r\" % (X.shape, y.shape)) \n",
        "print(\"classes: %r\" % list(np.unique(y)))\n",
        "\n",
        "n_samples, n_features = X.shape \n",
        "print(\"n_samples=%d\" % n_samples) \n",
        "print(\"n_features=%d\" % n_features)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXRAp-3euA6b"
      },
      "source": [
        "##standarziation of the features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2yLfISqvwPHl"
      },
      "source": [
        "\n",
        "def plot_gallery(data, labels, shape, interpolation='nearest'):\n",
        "   for i in range(data.shape[0]):\n",
        "     py.subplot(1, data.shape[0], (i + 1)) \n",
        "     py.imshow(data[i].reshape(shape), interpolation=interpolation) \n",
        "     py.title(labels[i])\n",
        "     py.xticks(()), py.yticks(()) \n",
        "     py.gray()\n",
        "\n",
        "subsample = np.random.permutation(X.shape[0])[:5] \n",
        "images = X[subsample]\n",
        "labels = ['True class: %d' % l for l in y[subsample]] \n",
        "plot_gallery(images, labels, shape=(8, 8))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SuMv3gUry5dT"
      },
      "source": [
        "!pip install RandomizedPCA\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3I0vFBiwur8"
      },
      "source": [
        "# 1. PCA on digits for visualization\n",
        "from sklearn.decomposition import PCA as RandomizedPCA\n",
        "pca = RandomizedPCA(n_components=5)\n",
        "X_pca = pca.fit_transform(X)\n",
        "X_pca.shape\n",
        "\n",
        "from itertools import cycle\n",
        "\n",
        "colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k']\n",
        "markers = ['+', 'o', '^', 'v', '<', '>', 'D', 'h', 's']\n",
        "for i, c, m in zip(np.unique(y), cycle(colors), cycle(markers)): \n",
        "  py.scatter(X_pca[y == i, 0], X_pca[y == i, 1],\n",
        "             c=c, marker=m, label=i, alpha=0.5)\n",
        "\n",
        "_ = py.legend(loc='best')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQRI9uTVCupS"
      },
      "source": [
        "print(pca)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Gkj98Hr5spc"
      },
      "source": [
        "labels = ['Component #%d' % i for i in range(len(pca.components_))] \n",
        "plot_gallery(pca.components_, labels, shape=(8, 8))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3ALfbuc1MBC"
      },
      "source": [
        "#task 1 part a \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LB3UZ09LIaAZ"
      },
      "source": [
        "#specified way in the assignment \n",
        " print(np.sum(pca.explained_variance_ratio_))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ifLvQW7Is2o0"
      },
      "source": [
        "#another way to calculate the explained variance \n",
        "import numpy\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "explained_variance = numpy.var(X_pca, axis=0)\n",
        "explained_variance_ratio = explained_variance / numpy.sum(explained_variance)\n",
        "numpy.cumsum(explained_variance_ratio)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfqPmQis8FgL"
      },
      "source": [
        "print('Explained Variance ', explained_variance)\n",
        "print('Explained Variance ratio ', explained_variance_ratio)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVRKGZFxD6va"
      },
      "source": [
        "#task 2 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CPYMqckDoO_"
      },
      "source": [
        "#Explained Variance ratio tells us the extent to which each component explains the original dataset.\n",
        "#so the 1st component is able to explain ~29% of X \n",
        "# second component is able to explain 23% of X\n",
        "#third component explains 20 percent \n",
        "#4th componet explains 15 % \n",
        "#5th component explains 11% \n",
        "#Together they can explain about 98% of the variance of X "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KeIU_wkwGXmR"
      },
      "source": [
        "#So if we only needed a 11% variance, we actually need just one component, let's verify\n",
        "pca=RandomizedPCA(n_components=0.11)\n",
        "X_new=pca.fit_transform(X)\n",
        "print (X_new.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccFzbfdEG3_Y"
      },
      "source": [
        "#So if we only needed a 15% variance, we actually need two component, let's verify\n",
        "pca=RandomizedPCA(n_components=0.15)\n",
        "X_new=pca.fit_transform(X)\n",
        "print (X_new.shape) # and so on "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aefzyRxKH5Oq"
      },
      "source": [
        "#Let's run PCA with 2 components so as to plot the data in 2D\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca_digits=PCA(n_components=5)\n",
        "X_proj = pca_digits.fit_transform(X_pca)\n",
        "print (np.sum(pca_digits.explained_variance_ratio_))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oviyKEp3igZT"
      },
      "source": [
        "!apt install tabulate\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1zGX8mKy4hV"
      },
      "source": [
        "# 2. PCA on digits for compression\n",
        "import pandas as pd \n",
        "from tabulate import tabulate\n",
        "n = 8 # number of digits for demonstration \n",
        "dims = [1,2,3,5,10,20,40,64]\n",
        "image_compression_percent=[]\n",
        "percent_variance=[]\n",
        "print('compressed images of first',n,'digits') \n",
        "print('with this many PCA components:',dims)\n",
        "for d in dims:\t# dimensionality for compressed signal \n",
        "  pca = RandomizedPCA(n_components=d)\n",
        "  percent_variance.append( np.sum(pca_digits.explained_variance_ratio_))\n",
        "  image_compression_percent.append(d/64)\n",
        "  X_pca1=pca.fit_transform(X)\n",
        "  reduced_X = pca.transform(X[0:n]) # the reduced dimensionality \n",
        "  recovered_X = pca.inverse_transform(reduced_X)\n",
        "  py.figure()\n",
        "  plot_gallery(recovered_X, y[0:n], shape=(8, 8))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JlwnQoEqjcu"
      },
      "source": [
        "#task 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EndxX6vjqlPc"
      },
      "source": [
        "df = pd.DataFrame(columns=['no_of_pca_componentes','image_compression_percent','percent_variance'], data=zip(dims, image_compression_percent, percent_variance))\n",
        "print (df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7wwtLVNkRkw"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqWJthAs9dqK"
      },
      "source": [
        "#task 1 b "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5hHiswnI9gv"
      },
      "source": [
        "#specified way in assignment \n",
        "print(np.sum(pca.explained_variance_ratio_))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RAQ2D35u9faC"
      },
      "source": [
        "#pca=PCA().fit(X)\n",
        "import numpy\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "explained_variance = numpy.var(X_pca1, axis=0)\n",
        "explained_variance_ratio = explained_variance / numpy.sum(explained_variance)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7RVcYX8E39b"
      },
      "source": [
        "print('Explained Variance ', explained_variance)\n",
        "print('Explained Variance ratio ', explained_variance_ratio)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlKrP9DLHCff"
      },
      "source": [
        "# these all variance ratio's are also worked like in previous example "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4IWejcL6lur"
      },
      "source": [
        "# 3. PCA on digits improve classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0)\n",
        "\n",
        "print(\"train data shape: %r, train target shape: %r\"% (X_train.shape, y_train.shape)) \n",
        "print(\"test data shape: %r, test target shape: %r\"% (X_test.shape, y_test.shape))\n",
        "\n",
        "from sklearn.naive_bayes import GaussianNB \n",
        "model = GaussianNB().fit(X_train, y_train) \n",
        "train_score = model.score(X_train, y_train)\n",
        "print('training score (overfitting!):',train_score)\n",
        "\n",
        "test_score = model.score(X_test, y_test) \n",
        "print('test score:',test_score)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wj4_X72r71qp"
      },
      "source": [
        "# but now using PCA features instead of pixels directly!\n",
        "\n",
        "pca = RandomizedPCA(n_components=10) \n",
        "pca.fit(X_train)\n",
        "\n",
        "tX_train = pca.transform(X_train) \n",
        "tX_test = pca.transform(X_test)\n",
        "\n",
        "model = GaussianNB().fit(tX_train, y_train) \n",
        "train_score = model.score(tX_train, y_train) \n",
        "print('training score (overfitting!):',train_score)\n",
        "\n",
        "test_score = model.score(tX_test, y_test) \n",
        "print('test score:',test_score)\n",
        "\n",
        "from sklearn import metrics \n",
        "y_test_pred = model.predict(tX_test) \n",
        "expected = y_test\n",
        "predicted = model.predict(tX_test)\n",
        "print(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(expected, predicted))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7KSGigVo8IKi"
      },
      "source": [
        "# let's plot accuracy vs number of components!\n",
        "\n",
        "accuracy =  [] \n",
        "n_comp = range(1,64) \n",
        "for i in n_comp:\n",
        "  pca = RandomizedPCA(n_components=i) \n",
        "  pca.fit(X_train)\n",
        "  tX_train = pca.transform(X_train) \n",
        "  tX_test = pca.transform(X_test)\n",
        "  model = GaussianNB().fit(tX_train, y_train) \n",
        "  test_score = model.score(tX_test, y_test) \n",
        "  accuracy.append(test_score)\n",
        "\n",
        "py.plot(n_comp, accuracy) \n",
        "py.xlabel('number of PCA components') \n",
        "py.ylabel('digit recognition accuracy')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cle7sjq98XvU"
      },
      "source": [
        "# 4. K-means clustering on digits\n",
        "\n",
        "# identify 10 clusters (which should correspond to digits) \n",
        "from sklearn import cluster\n",
        "\n",
        "k_means = cluster.KMeans(n_clusters=10) \n",
        "k_means.fit(digits.data)\n",
        "\n",
        "print('true\t:',digits.target[::50]) \n",
        "print('kmeans:',k_means.labels_[::50])\n",
        "\n",
        "metrics.adjusted_rand_score(digits.target, k_means.labels_)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ws84avV-8gnq"
      },
      "source": [
        "dbscan = cluster.DBSCAN(eps = 24, min_samples = 20) \n",
        "dbscan.fit(digits.data)\n",
        "\n",
        "print('true\t:',digits.target[::50]) \n",
        "print('dbscan:',dbscan.labels_[::50])\n",
        "\n",
        "metrics.adjusted_rand_score(digits.target, dbscan.labels_)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aV4n_GUDQaf3"
      },
      "source": [
        "#task 4\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tmA7snyP5BQ"
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "# Setup arrays to store train and test accuracies\n",
        "neighbors = np.arange(1, 64)\n",
        "train_accuracy = np.empty(len(neighbors))\n",
        "test_accuracy = np.empty(len(neighbors))\n",
        "\n",
        "# Loop over different values of k\n",
        "for i, k in enumerate(neighbors):\n",
        "    # Setup a k-NN Classifier with k neighbors: knn\n",
        "    knn = KNeighborsClassifier(n_neighbors = k)\n",
        "\n",
        "    # Fit the classifier to the training data\n",
        "    knn.fit(X_train,y_train)\n",
        "    \n",
        "    #Compute accuracy on the training set\n",
        "    train_accuracy[i] = knn.score(X_train, y_train)\n",
        "\n",
        "    #Compute accuracy on the testing set\n",
        "    test_accuracy[i] = knn.score(X_test, y_test)\n",
        "\n",
        "# Generate plot\n",
        "plt.title('k-NN: Varying Number of Neighbors')\n",
        "plt.plot(neighbors, test_accuracy, label = 'Testing Accuracy')\n",
        "plt.plot(neighbors, train_accuracy, label = 'Training Accuracy')\n",
        "plt.legend()\n",
        "plt.xlabel('Number of Neighbors')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2h95ZEMQ9ie"
      },
      "source": [
        "#no its not possible to get maximum accuracy at 64 diemensions "
      ]
    }
  ]
}